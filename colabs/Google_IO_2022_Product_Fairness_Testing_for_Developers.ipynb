{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Google_IO_2022_Product_Fairness_Testing_for_Developers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2022 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ],
      "metadata": {
        "id": "8yI7r1V4K43j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google I/O 2022: Product Fairness Testing for Developers\n",
        "\n",
        "---\n",
        "**Note: if you want to run this notebook, you need to clone it: use *__\"File\" -> \"Save a copy in Drive\"__* to get your own copy that you can play with.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uRb_t3yvvZe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content\n",
        "\n",
        "This notebook was created for Google I/O workshop on product fairness testing.\n",
        "\n",
        "## [1. Load and explore examples in our dataset](#scrollTo=kKQAvjrvNnWO)\n",
        "\n",
        "\n",
        "## [2. Filter data using the sensitive terms dataset](#scrollTo=mKwIfGoFN_qK)\n",
        "\n",
        "\n",
        "## [3. Refine our analysis using interactions](#scrollTo=jPHSuAfCUCkk)"
      ],
      "metadata": {
        "id": "tTxo-SRaNhRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and explore examples in our dataset\n",
        "Let's load some example data to analyze. These examples are a sample of about 1.1k conversations from WikiDialog, a dataset of 11 million generated conversations."
      ],
      "metadata": {
        "id": "kKQAvjrvNnWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4adJk13u9e5"
      },
      "outputs": [],
      "source": [
        "#@title Import relevant libraries \n",
        "\n",
        "# These are standard Python libraries useful to load and filter data.\n",
        "import re\n",
        "import csv\n",
        "import collections\n",
        "import io\n",
        "import logging\n",
        "import random\n",
        "\n",
        "# Pandas and data_table to represent the data and view it nicely. \n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "\n",
        "# The datasets\n",
        "wiki_dialog_url = 'https://raw.githubusercontent.com/google/responsible-innovation/main/data/wiki_dialog.csv'\n",
        "sensitive_terms_url = 'https://raw.githubusercontent.com/google/responsible-innovation/main/data/sensitive_terms.csv'\n",
        "interaction_table_url = 'https://raw.githubusercontent.com/google/responsible-innovation/main/data/interaction_table.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "\n",
        "# functions\n",
        "\n",
        "def view(df, include_index=False):\n",
        "  \"\"\"Display a Pandas data frame as an easy to use data table.\"\"\"\n",
        "  view_table = data_table.DataTable(df, include_index=include_index,\n",
        "                                    max_columns=100, num_rows_per_page=10)\n",
        "  return view_table\n",
        "\n",
        "# Load worksheet.\n",
        "examples = pd.read_csv(wiki_dialog_url)\n",
        "\n",
        "# View data.\n",
        "view(examples[['pid', 'title', 'utterances']])"
      ],
      "metadata": {
        "id": "yeUT1W-VvHvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Filter data using the sensitive terms dataset\n",
        "Let's try and be a bit more targetted with our exploration using the Sensitive Terms dataset. The dataset consists of a taxonomy of sensitive terms and adjectives to aid fairness analysis."
      ],
      "metadata": {
        "id": "mKwIfGoFN_qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the Sensitive Terms dataset.\n",
        "\n",
        "sensitive_terms = pd.read_csv(sensitive_terms_url)\n",
        "\n",
        "view(sensitive_terms)"
      ],
      "metadata": {
        "id": "qi-z7Ed6wyeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implement matcher for sensitive terms.\n",
        "# Create a regex matcher for the terms in the dataset. We can\n",
        "#   use this matcher to efficiently find and extract terms\n",
        "#   from the dataset that appear in sentences.\n",
        "term_matcher = re.compile(r'\\b(' + '|'.join(\n",
        "    f'{term.lower()}' for term in sensitive_terms['term']) + r')\\b')\n",
        "\n",
        "def get_matched_terms(text):\n",
        "  return set(term_matcher.findall(text.lower()))\n",
        "\n",
        "example_sentence = \"He is an abusive man.\"  #@param {type:\"string\"}\n",
        "get_matched_terms(example_sentence)"
      ],
      "metadata": {
        "id": "0WvWNdCgOe2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter the dataset to rows matching sensitive terms.  \n",
        "\n",
        "def get_matched_terms_string(row):\n",
        "  \"\"\"A helper function to return the matched terms as a string.\"\"\"\n",
        "  matched_terms = get_matched_terms(row['utterances'])\n",
        "  return \", \".join(matched_terms)\n",
        "\n",
        "# Extend examples to include the matched terms as another column.\n",
        "#   (axis=1) means that we will apply the above function to each row.\n",
        "examples['matched_terms'] = examples.apply(get_matched_terms_string, axis=1)\n",
        "examples_filtered_by_terms = examples[examples['matched_terms'] != '']\n",
        "view(examples_filtered_by_terms[['pid', 'title', 'utterances', 'matched_terms']])"
      ],
      "metadata": {
        "id": "qfxJu6hbPwM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Refine the analysis using interactions\n",
        "One observation is that we need to refine our analysis by looking at interactions between the sensitive terms and adjectives. We include a table of interaction terms. \n"
      ],
      "metadata": {
        "id": "jPHSuAfCUCkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load sensitive-interaction table.\n",
        "interaction_table = pd.read_csv(interaction_table_url)\n",
        "interaction_table = interaction_table.set_index('Interaction Type')\n",
        "\n",
        "view(interaction_table, include_index=True)"
      ],
      "metadata": {
        "id": "X8oeODojR-OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implement matcher for sensitive interactions.\n",
        "\n",
        "# Each term can describe a sensitive characteristic or an adjective type.\n",
        "# Store a mapping of them here.\n",
        "sensitive_categories, adjective_categories = {}, {}\n",
        "for _, row in sensitive_terms.iterrows():\n",
        "  label = row['category']\n",
        "  if row['sub_cat']:\n",
        "    label += f\": {row['sub_cat']}\"\n",
        "  if row['sentiment'] != 'NULL':\n",
        "    label += f\"/{row['sentiment']}\"\n",
        "  adjective_categories[row['term'].lower()] = label\n",
        "  if row['sensitive_characteristic'] != \"NULL\":\n",
        "    sensitive_categories[row['term'].lower()] = row['sensitive_characteristic']\n",
        "\n",
        "# Convert the interaction table into an easier format to find.\n",
        "sensitive_interactions = set()\n",
        "for term1, row in interaction_table.items():\n",
        "  for term2, value in row.items():\n",
        "    if value == 'X':\n",
        "      sensitive_interactions.add((term1.strip(), term2.strip()))\n",
        "\n",
        "# Define a function to find interactions.\n",
        "def get_matched_interactions(matched_terms):      \n",
        "  \"\"\"Find interactions between the `matched_terms` that might be sensitive.\"\"\"\n",
        "  interactions = []\n",
        "  matched_terms = sorted(matched_terms)\n",
        "  for i, term1 in enumerate(matched_terms):\n",
        "    id1 = sensitive_categories.get(term1)\n",
        "    adj1 = adjective_categories.get(term1)\n",
        "    for term2 in matched_terms[i+1:]:\n",
        "      id2 = sensitive_categories.get(term2)\n",
        "      adj2 = adjective_categories.get(term2)\n",
        "      if (id1, adj2) in sensitive_interactions:\n",
        "        interactions.append(f'{id1} ({term1}) x {adj2} ({term2})')\n",
        "      elif (id2, adj1) in sensitive_interactions:\n",
        "        interactions.append(f'{id2} ({term2}) x {adj1} ({term1})')\n",
        "  return set(interactions)\n",
        "\n",
        "example = \"aggressive men\" #@param{type: 'string'}\n",
        "matched_terms = get_matched_terms(example)\n",
        "get_matched_interactions(matched_terms)"
      ],
      "metadata": {
        "id": "3E493OXgUCAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Separate the given and generated text.\n",
        "def get_generated_text(row):\n",
        "  generated_questions = []\n",
        "  for utterance in row['utterances'].split('\\n'):\n",
        "    if utterance.startswith(\"Q:\"):\n",
        "      generated_questions.append(utterance)\n",
        "  return \"\\n\".join(generated_questions)\n",
        "\n",
        "def get_given_text(row):\n",
        "  generated_questions = []\n",
        "  for utterance in row['utterances'].split('\\n'):\n",
        "    if utterance.startswith(\"A:\"):\n",
        "      generated_questions.append(utterance)\n",
        "  return \"\\n\".join(generated_questions)\n",
        "\n",
        "examples[\"generated_text\"] = examples.apply(get_generated_text, axis=1)\n",
        "examples[\"given_text\"] = examples.apply(get_given_text, axis=1)\n",
        "\n",
        "view(examples[['pid', 'title', 'generated_text', 'given_text']])"
      ],
      "metadata": {
        "id": "76pOpsHAxX35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter the dataset to rows that match sensitive interactions. \n",
        "# Filter rows that match any of the aforementioned terms.\n",
        "def get_matched_interactions_string(row):\n",
        "  generated_terms = get_matched_terms(row['generated_text'])\n",
        "  given_terms = get_matched_terms(row['given_text'])\n",
        "  generated_terms.difference_update(given_terms)\n",
        "  matched_interactions = get_matched_interactions(generated_terms)\n",
        "  return \", \".join(matched_interactions)\n",
        "\n",
        "examples[\"matched_interactions\"] = examples.apply(\n",
        "    get_matched_interactions_string, axis=1)\n",
        "examples_filtered_by_interactions = examples[\n",
        "  examples[\"matched_interactions\"] != \"\"]"
      ],
      "metadata": {
        "id": "BrQ48aNPYfcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Count the number of interactions.\n",
        "examples_filtered_by_interactions[\"pid\"].describe()"
      ],
      "metadata": {
        "id": "YHsRY0y-ev3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Let's look at the first 8 examples\n",
        "view(examples_filtered_by_interactions.head(n=8)[\n",
        "    ['pid', 'title', 'utterances', 'matched_terms', 'matched_interactions']])"
      ],
      "metadata": {
        "id": "4w1h2XeSe_g0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}